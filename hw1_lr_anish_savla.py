# -*- coding: utf-8 -*-
"""HW1-LR_Anish Savla.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mUfEHbS8K7a85DOgcODsJjLwqZYne-by
"""

import torch
from torchtext.legacy import data
import numpy as np
import random
from torchtext.legacy import datasets
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm
import time
import pandas as pd

SEED = 11
pad_index = 1 
MAX_VOCAB_SIZE = 12000 
N_EPOCHS = 5
lr = 0.001 
bs = 528

random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True

TEXT = data.Field(tokenize = 'spacy',tokenizer_language = 'en_core_web_sm')
LABEL = data.LabelField(dtype = torch.float)


train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)
print(f'Number of training examples: {len(train_data)}')
print(f'Number of testing examples: {len(test_data)}')


train_data, valid_data = train_data.split(random_state = random.seed(SEED))
print(f'Number of training examples: {len(train_data)}')
print(f'Number of validation examples: {len(valid_data)}')
print(f'Number of testing examples: {len(test_data)}')

TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)
LABEL.build_vocab(train_data)
print(f"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}")
print(f"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}")

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class Dataset(Dataset):
    def __init__(self, data, text_vocab, label_vocab):
        self.X = [torch.LongTensor([text_vocab[word] for word in row.text]) for row in data]
        self.y = torch.FloatTensor([label_vocab[row.label] for row in data])
        self.lengths = torch.LongTensor([len(row.text) for row in data])
        
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self,index):
        return self.X[index],self.y[index], self.lengths[index]
    
def collate(batch):
    sent_len = [len(row[0]) for row in batch]
    sents = []
    for i in range(len(batch)):
        pad_vector = torch.LongTensor([pad_index]*(max(sent_len)-sent_len[i]))
        sents.append(torch.cat([batch[i][0],pad_vector]))
    sents = torch.stack(sents)
    labels = torch.stack([row[1] for row in batch])
    lens = torch.stack([row[2] for row in batch])
    return sents,labels,lens

def binary_accuracy(preds, y):
    """
    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8
    """
    #round predictions to the closest integer
    rounded_preds = torch.round(torch.sigmoid(preds))
    correct = (rounded_preds == y).float() #convert into float for division 
    acc = correct.sum() / len(correct)
    return acc

def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

class LR(nn.Module):
    def __init__(self, input_dim, embedding_dim, output_dim):
        super().__init__()
        self.embedding = nn.Embedding(input_dim, embedding_dim)
        self.fc = nn.Linear(embedding_dim, output_dim)
    def forward(self, text):
        embedded = self.embedding(text).squeeze().sum(0)
        return self.fc(embedded).squeeze()

EMBEDDING_DIM = 100
OUTPUT_DIM = 1

model = LR(len(TEXT.vocab), EMBEDDING_DIM, OUTPUT_DIM)
print (model)
print(f'The model has {count_parameters(model):,} trainable parameters')

def train(model, iterator, optimizer, criterion):
    epoch_loss = 0
    epoch_acc = 0
    model.train()
    for batch in tqdm(iterator, desc="Training...", total=len(iterator)):
        optimizer.zero_grad()
        x = batch[0].transpose(0,1).to(device)
        y =  batch[1].to(device)
        lens =  batch[2]
        predictions = model(x)
        loss = criterion(predictions, y)
        acc = binary_accuracy(predictions, y)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
        epoch_acc += acc.item()
    loss = epoch_loss / len(iterator)
    acc = epoch_acc / len(iterator)
    return loss, acc

def evaluate(model, iterator, criterion):
    epoch_loss = 0
    epoch_acc = 0
    model.eval()
    with torch.no_grad():
        for batch in tqdm(iterator, desc="Validation...", total=len(iterator)):
            x = batch[0].transpose(0,1).to(device)
            y = batch[1].to(device)
            lens = batch[2]
            predictions = model(x)
            loss = criterion(predictions, y)
            acc = binary_accuracy(predictions, y)
            epoch_loss += loss.item()
            epoch_acc += acc.item()
    loss = epoch_loss / len(iterator)
    acc = epoch_acc / len(iterator)
    return loss, acc

def epoch_time(start_time, end_time):
    elapsed_time = end_time - start_time
    elapsed_mins = int(elapsed_time / 60)
    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))
    return elapsed_mins, elapsed_secs

dataset_train = Dataset(train_data,TEXT.vocab.stoi, LABEL.vocab.stoi)
dataloader_train = DataLoader(dataset=dataset_train, collate_fn=collate, batch_size=bs)

dataset_val = Dataset(valid_data,TEXT.vocab.stoi, LABEL.vocab.stoi)
dataloader_val = DataLoader(dataset=dataset_val, collate_fn=collate, batch_size=bs)

dataset_test = Dataset(test_data,TEXT.vocab.stoi, LABEL.vocab.stoi)
dataloader_test = DataLoader(dataset=dataset_test, collate_fn=collate, batch_size=bs)

print ('Batch-size',bs)
model = LR(len(TEXT.vocab), EMBEDDING_DIM, OUTPUT_DIM)
criterion = nn.BCEWithLogitsLoss()
model = model.to(device)
criterion = criterion.to(device)
optimizer = optim.Adam(model.parameters())
best_valid_loss = float('inf')

epoch_train_loss_list=[]
epoch_train_acc_list=[]
epoch_val_loss_list=[]
epoch_val_acc_list=[]

for epoch in range(N_EPOCHS):
    start_time = time.time()
    train_loss, train_acc = train(model, dataloader_train, optimizer, criterion)
    valid_loss, valid_acc = evaluate(model, dataloader_val, criterion)
    end_time = time.time()
    epoch_mins, epoch_secs = epoch_time(start_time, end_time)
    if valid_loss < best_valid_loss:
        best_valid_loss = valid_loss
        torch.save(model.state_dict(), 'lr-model.pt')
    epoch_train_loss_list.append(train_loss)
    epoch_train_acc_list.append(train_acc)
    epoch_val_loss_list.append(valid_loss)
    epoch_val_acc_list.append(valid_acc)
    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')
    print(f'\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')
    print(f'\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')

model.load_state_dict(torch.load('lr-model.pt'))
test_loss, test_acc = evaluate(model, dataloader_test, criterion)
print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')

def prediction(model, iterator):
    model.eval()
    with torch.no_grad():
        gold_label=[]
        pred_label=[]
        for batch in tqdm(iterator, desc="Prediction...", total=len(iterator)):
            x,y,lens = batch[0].transpose(0,1).to(device), batch[1].to(device), batch[2]
            predictions = model(x)
            rounded_preds = torch.round(torch.sigmoid(predictions))
            pred_label.append(LABEL.vocab.itos[int(rounded_preds)])
            gold_label.append(LABEL.vocab.itos[int(y)])
    return pred_label,gold_label



dataset_val = Dataset(valid_data,TEXT.vocab.stoi, LABEL.vocab.stoi)
dataloader_val = DataLoader(dataset=dataset_val, collate_fn=collate, batch_size=1,shuffle=False)
dataset_test = Dataset(test_data,TEXT.vocab.stoi, LABEL.vocab.stoi)
dataloader_test = DataLoader(dataset=dataset_test, collate_fn=collate, batch_size=1,shuffle=False)

model = model.to(device)

val_pred,val_gold = prediction(model, dataloader_val)
test_pred, test_gold = prediction(model, dataloader_test)

valid_sents=[]
valid_gold=[]
valid_pred=[]
for i in range(len(valid_data)):
    valid_sents.append((' ').join(vars(valid_data[i])['text']))
    valid_gold.append(vars(valid_data[i])['label'])

validation_dict = {'Sentences':valid_sents,'Gold_labels':valid_gold,'Predictions':val_pred}
validation_df = pd.DataFrame(validation_dict)
validation_df.to_csv("LR_predictions_ValidationData.csv",index=False)



test_sents=[]
test_gold=[]
test_pred=[]
for i in range(len(test_data)):
    test_sents.append((' ').join(vars(test_data[i])['text']))
    test_gold.append(vars(test_data[i])['label'])

test_dict = {'Sentences':test_sents,'Gold_labels':test_gold,'Predictions':test_pred}
test_df = pd.DataFrame(validation_dict)
test_df.to_csv("LR_predictions_TestData.csv",index=False)